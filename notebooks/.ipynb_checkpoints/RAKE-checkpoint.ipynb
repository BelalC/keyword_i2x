{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from os import path\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/Belal/Projects/jobs/i2x_job/script.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ====== KEYWORD EXTRACTION ======\n",
    "# ================================\n",
    "stopword = \"stop_words/sklearn_stopwords.txt\"\n",
    "input = \"/Users/Belal/Projects/jobs/i2x_job/script.txt\"\n",
    "\n",
    "# Initialize RAKE object,\n",
    "rake_object = Rake(stop_words_path=stopword, min_char_length=5,\n",
    "                   max_words_length=3, min_keyword_frequency=4)\n",
    "\n",
    "# 2. run on RAKE on a given text\n",
    "sample_file = open(input, 'r', encoding=\"iso-8859-1\")\n",
    "text = sample_file.read()\n",
    "\n",
    "keywords = rake_object.run(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Word2Vec model...\n",
      "loaded model!\n"
     ]
    }
   ],
   "source": [
    "# ======= KEYWORD RANKING ========\n",
    "# ================================\n",
    "\n",
    "model = \"w2v_models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "print(\"loading Word2Vec model...\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "print(\"loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_feature_vecs([test_docs] ,model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1\n",
      "Review 0 of 1\n",
      "Review 0 of 1\n"
     ]
    }
   ],
   "source": [
    "test = \"evaluation/\"\n",
    "\n",
    "test_dirs = glob(path.join(test, \"*txt\"))\n",
    "test_docs = [doc.read() for doc in [open(test_file, \"r\") for test_file in test_dirs]]\n",
    "test_vecs = [get_avg_feature_vecs([doc],\n",
    "                                  model=model,\n",
    "                                  num_features=model.vector_size)\n",
    "             for doc in test_docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3193.11181641,  2132.89648438,    -5.62496948,  2498.46875   ,\n",
       "       -1009.86804199,   289.55325317, -1689.80322266, -1049.11242676,\n",
       "        -666.45129395,   428.36975098, -1054.78796387, -1627.27441406,\n",
       "       -3691.68603516,   506.66766357, -2325.35400391,  1676.30212402,\n",
       "        1861.67431641,  3143.71875   ,  -292.78210449,   120.52070618,\n",
       "       -4665.56640625,  -644.64984131,  2085.19287109,   296.72491455,\n",
       "       -1566.43151855,   531.07550049, -4607.33544922,  1040.11303711,\n",
       "        -517.31994629,  -380.19915771,  -194.37350464,   536.14135742,\n",
       "       -1413.38378906, -2052.39160156, -2510.00756836,  1775.68835449,\n",
       "       -3954.99487305,  2293.77880859, -1133.16723633,  1587.45471191,\n",
       "        -668.28808594,  -899.00054932,  1100.43359375,  1642.91455078,\n",
       "         927.42041016,  -452.34075928,  -594.91210938, -3238.29833984,\n",
       "       -2112.20092773,  1532.01831055, -3279.25292969,  4382.36621094,\n",
       "        -546.29931641,  4216.60107422,   817.12548828,  2493.01635742,\n",
       "       -2893.09765625, -1916.31982422,   -89.95082092, -2870.59472656,\n",
       "       -2521.78393555, -1322.29003906, -3515.35864258, -1002.54699707,\n",
       "        -745.57995605, -3632.70410156, -1835.30529785,  2206.83618164,\n",
       "       -1080.04663086,   839.62585449,   524.00091553, -1341.78515625,\n",
       "         671.80749512,  -316.80584717,  -543.19702148,  -184.9153595 ,\n",
       "        2450.33666992,   278.20022583,   420.37088013, -1581.48486328,\n",
       "       -2585.35839844,  -192.55581665,  -679.45861816,   -38.09625244,\n",
       "        2383.36035156,  1286.43103027,  -367.4786377 ,  4080.83618164,\n",
       "         335.44116211,   994.06072998,   373.13342285,   533.52026367,\n",
       "        -616.59619141, -2278.50195312,   878.14978027,  1941.27661133,\n",
       "       -1659.98010254,  1823.09924316,  4659.14013672,  -287.46191406,\n",
       "       -1371.09692383,   -92.46264648, -1109.56201172,  -294.79873657,\n",
       "       -1031.0246582 ,  2623.12084961, -1487.66625977,   996.11218262,\n",
       "        -150.88015747,   306.13397217, -2551.03149414, -3313.01855469,\n",
       "        -747.69958496,  -937.44317627,  1199.18640137,  2920.59570312,\n",
       "        1491.57287598,  -737.69226074,  -369.22299194,  -623.11297607,\n",
       "        2109.51391602,  1309.76696777,  -237.03744507,   443.25317383,\n",
       "        2216.72802734, -3001.95581055, -1812.27270508,  -345.6361084 ,\n",
       "        1094.48693848,  1957.01196289, -1332.32702637,  -422.36141968,\n",
       "       -1446.01135254,  -848.29193115, -3019.59008789,  1340.77734375,\n",
       "        -708.85180664,   630.51507568,  4411.29785156,  2567.0078125 ,\n",
       "        3672.73095703, -1304.40759277,   861.9163208 ,  -705.31713867,\n",
       "       -1126.24157715, -1774.40698242,  -941.3046875 ,  3006.10083008,\n",
       "        -979.74328613,   580.85882568,  1252.2199707 , -5341.47558594,\n",
       "        -898.62561035,  -789.81005859, -2050.00537109, -1634.41418457,\n",
       "        1390.27600098,  3234.8125    ,  -441.81048584,  -571.37750244,\n",
       "         399.21502686,  1594.57495117,    26.5831604 ,   123.36439514,\n",
       "        -338.77526855,  -908.03027344,  3030.05859375,   749.05419922,\n",
       "       -1872.34216309,  1632.21508789, -2064.81201172, -1705.51403809,\n",
       "        -734.80975342, -1378.21801758,  -333.80914307,  4143.40917969,\n",
       "        4635.29394531, -3869.17529297,   218.38027954, -1591.50537109,\n",
       "         137.72470093,   462.16595459,   842.92565918,  -772.10235596,\n",
       "        -433.66339111,  1355.53271484,  -742.76257324,  1262.32568359,\n",
       "        -490.1675415 ,   680.47937012,  2164.21728516,  -954.53320312,\n",
       "       -4490.44775391,  -377.88870239,  2153.08911133,   783.5425415 ,\n",
       "       -1194.4498291 ,  -225.69979858,  1262.67089844, -1860.14916992,\n",
       "       -2027.57531738,   383.88095093, -3710.83935547, -2379.81396484,\n",
       "        1201.42333984, -1356.0826416 ,  -596.04290771,  1981.05322266,\n",
       "        -753.6270752 ,  2962.23364258,  2675.63525391,  1593.25598145,\n",
       "       -2656.22607422,   103.84896851, -1568.53259277,  1022.1862793 ,\n",
       "        2926.7109375 , -1161.88439941, -2880.45385742,    17.77093506,\n",
       "       -2163.08618164,  2449.45361328,  1457.89257812,  -261.88671875,\n",
       "         161.7756958 , -1433.01269531,  1078.76147461,  1117.71679688,\n",
       "         130.74960327, -2619.28393555,   431.17547607,  -914.64978027,\n",
       "       -1677.76135254,   262.26898193,   171.86212158,   413.20489502,\n",
       "        1032.17211914,  1611.27441406,  1573.22558594,  1789.51574707,\n",
       "        1628.61328125,  1606.88354492,  4888.79394531, -1110.80175781,\n",
       "        -211.71246338,  1545.95300293, -2776.79052734,  2777.21020508,\n",
       "         922.58117676, -1384.19812012,  1180.73388672,   467.90731812,\n",
       "        1031.99609375,  1823.17944336,  -246.91387939, -2230.44140625,\n",
       "        -722.87969971,   979.25042725,   646.89404297, -1543.60107422,\n",
       "        1358.41833496,  -372.14562988,  1807.40441895,  -302.50897217,\n",
       "       -1569.41186523,   399.6892395 ,    96.5213623 ,   146.48069763,\n",
       "        -218.46366882, -1113.54199219, -1545.00439453,  -363.00976562,\n",
       "         797.65246582,   261.51367188,  1388.35131836, -1476.90148926,\n",
       "       -3195.21362305, -2484.12817383, -1825.03076172,  1296.81433105,\n",
       "       -1807.43554688,  2801.49829102,   -28.73397827,   125.85220337,\n",
       "          99.55349731,  1147.25390625,  -119.74285889,  -304.63931274,\n",
       "       -1052.06018066,  -893.42700195,  1315.87023926,   -56.41821289,\n",
       "       -1788.14697266,  1814.83764648,  -620.51715088, -2996.87329102,\n",
       "       -2048.20605469,  -434.65499878, -2035.68505859,  2802.39941406], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ========= SAVE OUTPUT ==========\n",
    "# ================================\n",
    "\n",
    "print(\"saving results\")\n",
    "\n",
    "with open(args[\"output\"], \"wb\") as outdir:\n",
    "    outdir.write(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase_list = []\n",
    "\n",
    "for s in t1_sent[0:5]:\n",
    "    tmp = re.sub(\" a \", '|', s.strip())\n",
    "    phrases = tmp.split(\"|\")\n",
    "    for phrase in phrases:\n",
    "        phrase = phrase.strip().lower()\n",
    "        if phrase != \"\" and is_acceptable(phrase, min_char_length=1, max_words_length=5):\n",
    "            phrase_list.append(phrase)\n",
    "\n",
    "phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_candidate_keywords(t1_sent, stop, min_char_length=2, max_words_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentenceList = split_sentences(data)\n",
    "stopwordpattern = build_stop_word_regex(stop)\n",
    "phraseList = generate_candidate_keywords(sentenceList, stopwordpattern, max_words_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordscores = calculate_word_scores(phraseList)\n",
    "keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedKeywords = sorted(keywordcandidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "totalKeywords = len(sortedKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for keyword in sortedKeywords[0:(totalKeywords // 3)]:\n",
    "    print (\"Keyword: \", keyword[0], \", score: \", keyword[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/Belal/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "stop = time.time()\n",
    "print(\"took {} to load\".format(start-stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/zelandiya/RAKE-tutorial/blob/master/rake.py\n",
    "\n",
    "with open(\"scripts/script.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "stop = \"/Users/Belal/Projects/jobs/i2x_job/RAKE-tutorial/stop_words/scikit_stopwords.txt\"\n",
    "\n",
    "rake_object = Rake(stop, 3, 1, 3)\n",
    "keywords = rake_object.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = model.word_vec(\"Potter\")\n",
    "e = model.word_vec(\"encyclopedia\")\n",
    "p_e = model.word_vec(\"Potter_encyclopedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_vec = []\n",
    "[key_vec.extend(model.word_vec(word[0])) for word in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_words = generate_candidate_keywords(split_sentences(t1), stopword_pattern=stop, min_char_length=2, max_words_length=2)\n",
    "test2_words = generate_candidate_keywords(split_sentences(t2), stopword_pattern=stop, min_char_length=2, max_words_length=2)\n",
    "test3_words = generate_candidate_keywords(split_sentences(t3), stopword_pattern=stop, min_char_length=2, max_words_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "bool_split = [word in model.vocab for word in test_words]\n",
    "x = list(compress(test_words, bool_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lll = []\n",
    "[lll.append(model.word_vec(y)) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# my own Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        #Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        #Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list = [test_words, test2_words, test3_words]\n",
    "trainDataVecs = [getAvgFeatureVecs([test_doc], model=model, num_features=model.vector_size) for test_doc in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_test = (trainDataVecs[0] + trainDataVecs[1] + trainDataVecs[2])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=(x[0] + x[1] + x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sum = ([sum(x) for x in zip(x[0], x[1], x[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for test_vec in trainDataVecs:\n",
    "    for key_word in key_vec:\n",
    "        x.append(pairwise.cosine_similarity(X=key_word.reshape(1,-1), Y = test_vec.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for test_vec in trainDataVecs:\n",
    "    x.append([pairwise.cosine_similarity(X=key_word.reshape(1,-1), Y = test_vec.reshape(1,-1)) for key_word in key_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newList = [x / 3 for x in x_sum]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_key = [k[0] for k in keywords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = list(zip(names_key, newList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fff=[]\n",
    "for i in range(len(names_key)):\n",
    "    fff.append((names_key[i], newList[i][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked = sorted(fff, key=lambda x: x[1], reverse=True)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in ttt[0:1]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Implementation of RAKE - Rapid Automatic Keyword Extraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010).\n",
    "# Automatic keyword extraction from individual documents.\n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "#\n",
    "# NOTE: The original implementation (available at - https://github.com/zelandiya/RAKE-tutorial)\n",
    "# has been extended and updated to work with Python 3 and to include more specific functionality\n",
    "\n",
    "\n",
    "import re\n",
    "import operator\n",
    "import six\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "# Required functions for RAKE\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = '\\\\b' + word + '\\\\b'\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern, min_char_length=1, max_words_length=5):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\" and is_acceptable(phrase, min_char_length, max_words_length):\n",
    "                phrase_list.append(phrase)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def is_acceptable(phrase, min_char_length, max_words_length):\n",
    "\n",
    "    # a phrase must have a min length in characters\n",
    "    if len(phrase) < min_char_length:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have a max number of words\n",
    "    words = phrase.split()\n",
    "    if len(words) > max_words_length:\n",
    "        return 0\n",
    "\n",
    "    digits = 0\n",
    "    alpha = 0\n",
    "    for i in range(0, len(phrase)):\n",
    "        if phrase[i].isdigit():\n",
    "            digits += 1\n",
    "        elif phrase[i].isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    # a phrase must have at least one alpha character\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have more alpha than digits characters\n",
    "    if digits > alpha:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        # if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  # orig.\n",
    "            # word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/freq(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    # word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score, min_keyword_frequency=1):\n",
    "    keyword_candidates = {}\n",
    "\n",
    "    for phrase in phrase_list:\n",
    "        if min_keyword_frequency > 1:\n",
    "            if phrase_list.count(phrase) < min_keyword_frequency:\n",
    "                continue\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path, min_char_length=1, max_words_length=5, min_keyword_frequency=1):\n",
    "        self.__stop_words_path = stop_words_path\n",
    "        self.__stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "        self.__min_char_length = min_char_length\n",
    "        self.__max_words_length = max_words_length\n",
    "        self.__min_keyword_frequency = min_keyword_frequency\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern,\n",
    "                                                  self.__min_char_length, self.__max_words_length)\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores, self.__min_keyword_frequency)\n",
    "        sorted_keywords = sorted(six.iteritems(keyword_candidates), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "\n",
    "test=None\n",
    "# Testing + debugging RAKE on pre-defined text block\n",
    "if test:\n",
    "    text = \"Compatibility of systems of linear constraints over the set of natural numbers. \" \\\n",
    "           \"Criteria of compatibility of a system of linear Diophantine equations, strict inequations,\" \\\n",
    "           \" and nonstrict inequations are considered. Upper bounds for components of a minimal set of \" \\\n",
    "           \"solutions and algorithms of construction of minimal generating sets of solutions for all types\" \\\n",
    "           \" of systems are given. These criteria and the corresponding algorithms for constructing a minimal\" \\\n",
    "           \" supporting set of solutions can be used in solving all the considered\" \\\n",
    "           \" types of systems and systems of mixed types.\"\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    stoppath = \"stop_words/sklearn_stopwords.txt\"\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug:\n",
    "        print(keywordcandidates)\n",
    "\n",
    "    sortedKeywords = sorted(six.iteritems(keywordcandidates), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug:\n",
    "        print(sortedKeywords)\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug:\n",
    "        print(totalKeywords)\n",
    "        print(sortedKeywords[0:(totalKeywords // 3)])\n",
    "\n",
    "    rake = Rake(\"stop_words/sklearn_stopwords.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Function to average all of the word vectors in a given paragraph\n",
    "    :param words:\n",
    "    :param model:\n",
    "    :param num_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "\n",
    "    n_words = 0\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    #\n",
    "    # Pre-allocate a 2D numpy array, for speed\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message\n",
    "        # if counter % 1000 == 0:\n",
    "        print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "    return review_feature_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

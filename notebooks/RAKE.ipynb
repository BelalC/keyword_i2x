{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from os import path\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/Belal/Projects/jobs/i2x_job/keyword_xtract/script.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ====== KEYWORD EXTRACTION ======\n",
    "# ================================\n",
    "stopword = \"/Users/Belal/Projects/jobs/i2x_job/keyword_xtract/stop_words/sklearn_stopwords.txt\"\n",
    "inputt = \"/Users/Belal/Projects/jobs/i2x_job/keyword_xtract/script.txt\"\n",
    "\n",
    "# Initialize RAKE object,\n",
    "rake_object = Rake(stop_words_path=stopword, min_char_length=4,\n",
    "                   max_words_length=4, min_keyword_frequency=3)\n",
    "\n",
    "# 2. run on RAKE on a given text\n",
    "sample_file = open(inputt, 'r')\n",
    "text = sample_file.read()\n",
    "\n",
    "keywords = rake_object.run(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Word2Vec model...\n",
      "loaded model!\n"
     ]
    }
   ],
   "source": [
    "# ======= KEYWORD RANKING ========\n",
    "# ================================\n",
    "\n",
    "model = \"/Users/Belal/Projects/jobs/i2x_job/keyword_xtract/w2v_models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "print(\"loading Word2Vec model...\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "print(\"loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = \"evaluation/\"\n",
    "\n",
    "test_dirs = glob(path.join(test, \"*txt\"))\n",
    "test_docs = [doc.read() for doc in [open(test_file, \"r\") for test_file in test_dirs]]\n",
    "test_vecs = [get_avg_feature_vecs([doc],\n",
    "                                  model=model,\n",
    "                                  num_features=model.vector_size)\n",
    "             for doc in test_docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "index2word_set = set(model.index2word)\n",
    "bool_split = [word[0] in index2word_set for word in keywords]\n",
    "keyword_in_model = list(compress(keywords, bool_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort keywords + choose how many to use\n",
    "sorted_keyword = sorted(keyword_in_model, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_keyword = sorted(keyword_in_model, key=lambda x: x[1], reverse=True)\n",
    "n_keywords = int(0.25*len(sorted_keyword))\n",
    "keyword_list = sorted_keyword[0:n_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyword_vecs = [(model.word_vec(word[0])) for word in sorted_keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generating candidate words from test docs (optional)\n",
    "# test_words = generate_candidate_keywords(split_sentences(test_docs[0]), stopword_pattern=stopword,\n",
    "#                                         min_char_length=2, max_words_length=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1\n",
      "Review 0 of 1\n",
      "Review 0 of 1\n"
     ]
    }
   ],
   "source": [
    "test = \"../evaluation/\"\n",
    "\n",
    "test_dirs = glob(path.join(test, \"*txt\"))\n",
    "test_docs = [doc.read() for doc in [open(test_file, \"r\") for test_file in test_dirs]]\n",
    "test_vecs = [get_avg_feature_vecs([doc],\n",
    "                                  model=model,\n",
    "                                  num_features=model.vector_size)\n",
    "             for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ranking\n",
    "from sklearn.metrics import pairwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for vec in test_vecs:\n",
    "    for key_word in keyword_vecs:\n",
    "        x.append(pairwise.cosine_similarity(X=key_word.reshape(1,-1), Y = vec.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for vec in test_vecs:\n",
    "    x.append([pairwise.cosine_similarity(X=key_word.reshape(1,-1), Y = vec.reshape(1,-1)) for key_word in keyword_vecs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=np.zeros_like(x[0])\n",
    "for doc in x:\n",
    "    sum_keyword = z + doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_key = [k[0] for k in sorted_keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding cosine similarities to get a single 'rank' for each keyword\n",
    "z=np.zeros_like(x[0])\n",
    "for y in x:\n",
    "    z=z+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newlist = z/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = list(zip(names_key, newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fff=[]\n",
    "for i in range(len(names_key)):\n",
    "    fff.append((names_key[i], newlist[i][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('china', 0.242401),\n",
       " ('type', 0.19073217),\n",
       " ('means', 0.16486825),\n",
       " ('form', 0.13493739),\n",
       " ('rice', 0.12611552),\n",
       " ('chicken', 0.12589379),\n",
       " ('sweet', 0.11941203),\n",
       " ('world', 0.1109058),\n",
       " ('development', 0.10882666),\n",
       " ('having', 0.10622595),\n",
       " ('used', 0.10187316),\n",
       " ('cultural', 0.099895574),\n",
       " ('types', 0.097578488),\n",
       " ('umami', 0.097115934),\n",
       " ('health', 0.094237708),\n",
       " ('salt', 0.092982449),\n",
       " ('times', 0.092271946),\n",
       " ('starvation', 0.089643747),\n",
       " ('hunger', 0.089590617),\n",
       " ('minerals', 0.088807322),\n",
       " ('texture', 0.088739671),\n",
       " ('people', 0.086064778),\n",
       " ('result', 0.0837484),\n",
       " ('sugar', 0.082929388),\n",
       " ('methods', 0.080974534),\n",
       " ('known', 0.079273887),\n",
       " ('packaging', 0.075944103),\n",
       " ('fruits', 0.073765509),\n",
       " ('corn', 0.073155783),\n",
       " ('according', 0.071755178),\n",
       " ('increase', 0.071639679),\n",
       " ('transportation', 0.070788026),\n",
       " ('cooked', 0.070380159),\n",
       " ('wheat', 0.070036478),\n",
       " ('investment', 0.068769686),\n",
       " ('reasons', 0.068568893),\n",
       " ('affect', 0.068433747),\n",
       " ('culture', 0.067772917),\n",
       " ('consumption', 0.066966295),\n",
       " ('caused', 0.064567439),\n",
       " ('seeds', 0.062590219),\n",
       " ('fish', 0.061663259),\n",
       " ('enhance', 0.060794968),\n",
       " ('sale', 0.060116809),\n",
       " ('food', 0.05965361),\n",
       " ('agriculture', 0.059275296),\n",
       " ('sweetness', 0.058638304),\n",
       " ('home', 0.056022521),\n",
       " ('cultures', 0.054038893),\n",
       " ('vegetables', 0.053137455),\n",
       " ('seafood', 0.052252591),\n",
       " ('diet', 0.044206943),\n",
       " ('meat', 0.040651802),\n",
       " ('prepared', 0.040598195),\n",
       " ('grow', 0.040244192),\n",
       " ('usually', 0.039673906),\n",
       " ('consumers', 0.039646834),\n",
       " ('prices', 0.039328538),\n",
       " ('meats', 0.038419973),\n",
       " ('preparation', 0.036577865),\n",
       " ('eaten', 0.036285266),\n",
       " ('ovens', 0.036008809),\n",
       " ('enjoyable', 0.033106118),\n",
       " ('considered', 0.032849845),\n",
       " ('fats', 0.032445978),\n",
       " ('flavor', 0.032211341),\n",
       " ('farmers', 0.030326007),\n",
       " ('companies', 0.02906364),\n",
       " ('humans', 0.02753544),\n",
       " ('sour', 0.026070559),\n",
       " ('cooking', 0.02463785),\n",
       " ('countries', 0.021427276),\n",
       " ('ingredients', 0.02129437),\n",
       " ('example', 0.01869305),\n",
       " ('production', 0.015212964),\n",
       " ('taste', 0.015157831),\n",
       " ('bitter', 0.010554562),\n",
       " ('plants', 0.0068413611),\n",
       " ('foods', 0.0060227271),\n",
       " ('animal', 0.001508461),\n",
       " ('contaminants', -0.0070710517),\n",
       " ('fired', -0.0070932577),\n",
       " ('safety', -0.0070954538),\n",
       " ('addition', -0.0072883219),\n",
       " ('lead', -0.011358492),\n",
       " ('animals', -0.012626085),\n",
       " ('contrast', -0.02175129)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked = sorted(fff, key=lambda x: x[1], reverse=True)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = \"bla.txt\"\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results\n"
     ]
    }
   ],
   "source": [
    "# ========= SAVE OUTPUT ==========\n",
    "# ================================\n",
    "\n",
    "print(\"saving results\")\n",
    "\n",
    "with open(\"bla.txt\",\"w\") as f:\n",
    "    for line in ranked:\n",
    "        strs=\"    score: \".join(str(x) for x in line)\n",
    "        f.write(strs+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china    score: 0.242626\n",
      "type    score: 0.190607\n",
      "means    score: 0.164325\n",
      "form    score: 0.134714\n",
      "rice    score: 0.126557\n",
      "chicken    score: 0.125995\n",
      "sweet    score: 0.119496\n",
      "world    score: 0.112293\n",
      "development    score: 0.109133\n",
      "having    score: 0.107233\n",
      "used    score: 0.101601\n",
      "cultural    score: 0.100244\n",
      "types    score: 0.0964164\n",
      "umami    score: 0.0957589\n",
      "health    score: 0.0931618\n",
      "salt    score: 0.0921845\n",
      "times    score: 0.0915946\n",
      "hunger    score: 0.0897221\n",
      "starvation    score: 0.0894356\n",
      "texture    score: 0.0879902\n",
      "minerals    score: 0.087908\n",
      "people    score: 0.0856616\n",
      "result    score: 0.0828769\n",
      "sugar    score: 0.0823318\n",
      "methods    score: 0.0808169\n",
      "known    score: 0.0788293\n",
      "packaging    score: 0.0769205\n",
      "according    score: 0.0742263\n",
      "corn    score: 0.0741341\n",
      "fruits    score: 0.073292\n",
      "increase    score: 0.0727475\n",
      "wheat    score: 0.071175\n",
      "transportation    score: 0.0708261\n",
      "cooked    score: 0.0696251\n",
      "investment    score: 0.0691062\n",
      "reasons    score: 0.0690556\n",
      "culture    score: 0.0684923\n",
      "affect    score: 0.0679003\n",
      "consumption    score: 0.067041\n",
      "caused    score: 0.0646559\n",
      "seeds    score: 0.0631239\n",
      "sale    score: 0.0612348\n",
      "fish    score: 0.0610483\n",
      "enhance    score: 0.0608734\n",
      "agriculture    score: 0.0596838\n",
      "food    score: 0.0593474\n",
      "home    score: 0.0585603\n",
      "sweetness    score: 0.0582649\n",
      "cultures    score: 0.0544249\n",
      "vegetables    score: 0.0527228\n",
      "seafood    score: 0.0516737\n",
      "diet    score: 0.0426555\n",
      "prepared    score: 0.0418323\n",
      "prices    score: 0.0409436\n",
      "grow    score: 0.0405165\n",
      "consumers    score: 0.040495\n",
      "meat    score: 0.0399547\n",
      "usually    score: 0.0396568\n",
      "meats    score: 0.0376271\n",
      "preparation    score: 0.0364894\n",
      "eaten    score: 0.0349766\n",
      "ovens    score: 0.0345096\n",
      "enjoyable    score: 0.0339255\n",
      "considered    score: 0.03249\n",
      "flavor    score: 0.0313941\n",
      "fats    score: 0.0309461\n",
      "farmers    score: 0.0302418\n",
      "companies    score: 0.0292786\n",
      "humans    score: 0.0272249\n",
      "sour    score: 0.0259572\n",
      "cooking    score: 0.0245886\n",
      "countries    score: 0.0210818\n",
      "ingredients    score: 0.0203366\n",
      "example    score: 0.0191558\n",
      "production    score: 0.0160983\n",
      "taste    score: 0.0155592\n",
      "bitter    score: 0.0120033\n",
      "plants    score: 0.00684935\n",
      "foods    score: 0.00524395\n",
      "animal    score: 0.000727753\n",
      "fired    score: -0.00596238\n",
      "safety    score: -0.00715225\n",
      "contaminants    score: -0.00771302\n",
      "addition    score: -0.0079871\n",
      "lead    score: -0.0108352\n",
      "animals    score: -0.012787\n",
      "contrast    score: -0.021082\n"
     ]
    }
   ],
   "source": [
    "for line in ranked:\n",
    "    strs=\"    score: \".join(str(x) for x in line)\n",
    "    print(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Implementation of RAKE - Rapid Automatic Keyword Extraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010).\n",
    "# Automatic keyword extraction from individual documents.\n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "#\n",
    "# NOTE: The original implementation (available at - https://github.com/zelandiya/RAKE-tutorial)\n",
    "# has been extended and updated to work with Python 3 and to include more specific functionality\n",
    "\n",
    "\n",
    "import re\n",
    "import operator\n",
    "import six\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "# Required functions for RAKE\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = '\\\\b' + word + '\\\\b'\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern, min_char_length=1, max_words_length=5):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\" and is_acceptable(phrase, min_char_length, max_words_length):\n",
    "                phrase_list.append(phrase)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def is_acceptable(phrase, min_char_length, max_words_length):\n",
    "\n",
    "    # a phrase must have a min length in characters\n",
    "    if len(phrase) < min_char_length:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have a max number of words\n",
    "    words = phrase.split()\n",
    "    if len(words) > max_words_length:\n",
    "        return 0\n",
    "\n",
    "    digits = 0\n",
    "    alpha = 0\n",
    "    for i in range(0, len(phrase)):\n",
    "        if phrase[i].isdigit():\n",
    "            digits += 1\n",
    "        elif phrase[i].isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    # a phrase must have at least one alpha character\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have more alpha than digits characters\n",
    "    if digits > alpha:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        # if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  # orig.\n",
    "            # word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/freq(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    # word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score, min_keyword_frequency=1):\n",
    "    keyword_candidates = {}\n",
    "\n",
    "    for phrase in phrase_list:\n",
    "        if min_keyword_frequency > 1:\n",
    "            if phrase_list.count(phrase) < min_keyword_frequency:\n",
    "                continue\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path, min_char_length=1, max_words_length=5, min_keyword_frequency=1):\n",
    "        self.__stop_words_path = stop_words_path\n",
    "        self.__stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "        self.__min_char_length = min_char_length\n",
    "        self.__max_words_length = max_words_length\n",
    "        self.__min_keyword_frequency = min_keyword_frequency\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern,\n",
    "                                                  self.__min_char_length, self.__max_words_length)\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores, self.__min_keyword_frequency)\n",
    "        sorted_keywords = sorted(six.iteritems(keyword_candidates), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "\n",
    "test=None\n",
    "# Testing + debugging RAKE on pre-defined text block\n",
    "if test:\n",
    "    text = \"Compatibility of systems of linear constraints over the set of natural numbers. \" \\\n",
    "           \"Criteria of compatibility of a system of linear Diophantine equations, strict inequations,\" \\\n",
    "           \" and nonstrict inequations are considered. Upper bounds for components of a minimal set of \" \\\n",
    "           \"solutions and algorithms of construction of minimal generating sets of solutions for all types\" \\\n",
    "           \" of systems are given. These criteria and the corresponding algorithms for constructing a minimal\" \\\n",
    "           \" supporting set of solutions can be used in solving all the considered\" \\\n",
    "           \" types of systems and systems of mixed types.\"\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    stoppath = \"stop_words/sklearn_stopwords.txt\"\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug:\n",
    "        print(keywordcandidates)\n",
    "\n",
    "    sortedKeywords = sorted(six.iteritems(keywordcandidates), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug:\n",
    "        print(sortedKeywords)\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug:\n",
    "        print(totalKeywords)\n",
    "        print(sortedKeywords[0:(totalKeywords // 3)])\n",
    "\n",
    "    rake = Rake(\"stop_words/sklearn_stopwords.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Function to average all of the word vectors in a given paragraph\n",
    "    :param words:\n",
    "    :param model:\n",
    "    :param num_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "\n",
    "    n_words = 0\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    #\n",
    "    # Pre-allocate a 2D numpy array, for speed\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message\n",
    "        # if counter % 1000 == 0:\n",
    "        print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "    return review_feature_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
